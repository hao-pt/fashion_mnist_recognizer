{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_implementation_tensorflow.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6kcOKbIllLNf"},"source":["#  Fashion - MNIST dataset classification with CNN \n","\n","Phung Tien Hao\n","\n","---"]},{"cell_type":"code","metadata":{"id":"8ABo134Nak69","colab_type":"code","outputId":"6ef61f46-8cdf-4915-920e-1595a273174c","executionInfo":{"status":"ok","timestamp":1570448781107,"user_tz":-420,"elapsed":3116,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yYrLsOedsOHw","colab_type":"text"},"source":["Change dir"]},{"cell_type":"code","metadata":{"id":"RV7IQjFmavm2","colab_type":"code","outputId":"4b005e30-d3d6-4bd9-c701-bc51742d5b0a","executionInfo":{"status":"ok","timestamp":1570448784770,"user_tz":-420,"elapsed":6035,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["%cd '/content/gdrive/My Drive/Senior/Computer vision in application/Lab 01'\n","%cat requirements.txt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Senior/Computer vision in application/Lab 01\n","tensorflow-gpu==1.13.1\n","scipy==1.2.1\n","scikit-learn\n","opencv-python\n","h5py\n","matplotlib\n","Pillow\n","requests\n","psutil\n","pandas"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AE0Az_je7QkN","colab_type":"code","outputId":"e1252ea1-25fc-4921-ef82-f19aa32ad8f4","executionInfo":{"status":"ok","timestamp":1570448762497,"user_tz":-420,"elapsed":5280,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!pip install -r requirements.txt"],"execution_count":34,"outputs":[{"output_type":"stream","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n","The folder you are executing pip from can no longer be found.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zFeqUhfdlLNh"},"source":["# 1. Import lib"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uWNZJttylLNh","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from cnn_ultils import *\n","from datetime import datetime\n","import os\n","# Set seed\n","tf.set_random_seed(1)\n","np.random.seed(1)\n","\n","# Embedd image into notebook\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uuAwg0kLlLNm"},"source":["# 2. Load dataset"]},{"cell_type":"markdown","metadata":{"id":"f8BP_V06tmw8","colab_type":"text"},"source":["Class names are used for ploting"]},{"cell_type":"code","metadata":{"id":"TZsbYhFgtj2b","colab_type":"code","colab":{}},"source":["class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xqqQ214zlLNn","colab":{}},"source":["(X_train, Y_train), (X_test, Y_test) = load_data(\"fashionmnist/fashion-mnist_train.csv\", \"fashionmnist/fashion-mnist_test.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kju6ZMyLlLNp"},"source":["Explore dataset and shuffle dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VWNL0DbclLNp","outputId":"d2f37b16-2097-45f4-9130-54451b2fd3e6","executionInfo":{"status":"ok","timestamp":1570451619937,"user_tz":-420,"elapsed":2522,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Reshape training and testing image\n","X_train, X_test = X_train.reshape((-1, 28, 28, 1)), X_test.reshape((-1, 28, 28, 1))\n","Y_train, Y_test = convert_to_one_hot(Y_train, 10), convert_to_one_hot(Y_test, 10)\n","\n","(X_train, Y_train), (X_val, Y_val) = train_val_split(X_train, Y_train, val_size=10000)    \n","\n","# Normalize data x to be in range 0-1\n","X_train = X_train/255.0\n","X_val = X_val/255.0\n","X_test = X_test/255.0\n","\n","print(\"X_train:\", X_train.shape)\n","print(\"Y_train:\", Y_train.shape)\n","print(\"X_val:\", X_val.shape)\n","print(\"Y_val:\", Y_val.shape)\n","print(\"X_test:\", X_test.shape)\n","print(\"Y_test:\", Y_test.shape)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["X_train: (50000, 28, 28, 1)\n","Y_train: (50000, 10)\n","X_val: (10000, 28, 28, 1)\n","Y_val: (10000, 10)\n","X_test: (10000, 28, 28, 1)\n","Y_test: (10000, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sEC_rWrslLNr"},"source":["## Plot some training images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nkrASpa4lLNs","outputId":"38909f43-22f9-4804-a58c-e3dd2448a6fd","executionInfo":{"status":"error","timestamp":1570448809705,"user_tz":-420,"elapsed":2533,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":347}},"source":["# Plot 25 first examples from training set\n","plt.figure(figsize=(10, 10))\n","for i in range(25):\n","    plt.subplot(5, 5, i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X_train[i].reshape((28, 28)), cmap = \"binary\")\n","    plt.xlabel(class_names[np.argmax(Y_train[i])])\n","\n","plt.show()"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-9175cd7548f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAHMAAABzCAYAAACrQz3mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfuhaolQAAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"QMxZLawur0y1","colab_type":"text"},"source":["## Data augmentation"]},{"cell_type":"code","metadata":{"id":"pq4NSotwsbmK","colab_type":"code","colab":{}},"source":["def horizontal_flip(img):\n","    \"\"\"\n","    Horizontal Flip augmentation\n","\n","    Args:\n","        img: Input image\n","    Returns:\n","        Flip image\n","    \"\"\"\n","    img = tf.image.flip_left_right(img)\n","\n","    return img\n","\n","def crop(img):\n","    \"\"\"\n","    Random crop augmentation\n","\n","    Args:\n","        img: Input image\n","    Returns:\n","        Croped image\n","    \"\"\"\n","    img = tf.image.random_crop(img, size=[23, 23, 1]) # crop size = 0.8 * img size\n","    img = tf.image.resize_image_with_pad(img, 28, 28, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JG-Zu2Okr4S0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"0d68c10d-9cf0-4579-e3bf-fefb0645c7e5","executionInfo":{"status":"ok","timestamp":1570444454788,"user_tz":-420,"elapsed":34286,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}}},"source":["\"\"\"Data augmentation\"\"\"\n","rand_index = np.random.choice(50000, 10000, replace=False)\n","aug_dataset = X_train.copy()[rand_index]\n","aug_Y = Y_train.copy()[rand_index]\n","\n","augmentation_methods = [horizontal_flip, crop]\n","\n","aug_datasets = []\n","sess = tf.Session()\n","for f in augmentation_methods:\n","    # Apply an augmentation only in 50% of the dataset\n","    aug_dataset = tf.map_fn(lambda x: f(x), aug_dataset, parallel_iterations=10)\n","    aug_dataset = tf.map_fn(lambda x: tf.clip_by_value(x, 0, 1), aug_dataset) # Cut values out of range (0, 1)\n","    aug_datasets.append(sess.run(aug_dataset))\n","sess.close()\n","\n","aug_datasets = np.concatenate((aug_datasets[0], aug_datasets[1]), axis=0)\n","aug_Y = np.concatenate((aug_Y, aug_Y), axis=0)\n","print(aug_datasets.shape)\n","\n","X_train = np.concatenate((X_train, aug_datasets), axis=0)\n","Y_train = np.concatenate((Y_train, aug_Y), axis=0)\n","print(X_train.shape)\n","print(Y_train.shape)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["(20000, 28, 28, 1)\n","(70000, 28, 28, 1)\n","(70000, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dh1HymNHlLNu"},"source":["# 3. Create placeholders"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T-4tNqndlLNv","colab":{}},"source":["def create_placeholders(nH0, nW0, nC0, nY):\n","    '''\n","    Purpose: Create placeholders for input data. \n","    Construct place for input data that allows to specify values later\n","    \n","    Input:\n","        nH0: the height of input data (image)\n","        nW0: the width of input data (image)\n","        nC0: the number of channels of input data (image)\n","        nY: the number of channels of Y\n","      \n","    Output:\n","        X, Y: placeholders for input data\n","        keep_prob: use for dropout\n","        is_training: use for batchnorm\n","    '''\n","    X = tf.placeholder(dtype = tf.float32, shape = [None, nH0, nW0, nC0], name='input')\n","    Y = tf.placeholder(dtype = tf.float32, shape = [None, nY], name = 'label')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    is_training = tf.placeholder(tf.bool, name='is_training')\n","    return X, Y, keep_prob, is_training"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BoyDHgQulLNw","outputId":"8e5f4aa5-a784-4541-92a7-80592bf4688b","executionInfo":{"status":"ok","timestamp":1570448817536,"user_tz":-420,"elapsed":704,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["X, Y, keep_prob, is_training = create_placeholders(3, 3, 2, 1)\n","\n","print(X)\n","print(Y)\n","print(keep_prob)\n","print(is_training)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Tensor(\"input:0\", shape=(?, 3, 3, 2), dtype=float32)\n","Tensor(\"label:0\", shape=(?, 1), dtype=float32)\n","Tensor(\"keep_prob:0\", dtype=float32)\n","Tensor(\"is_training:0\", dtype=bool)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DcPrp5ndlLNy"},"source":["# 4. Initialize parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DRPpr0DelLNz","colab":{}},"source":["def init_params():\n","    # '''Init params to build neural network using He initialization\n","    # Model includes 2 CONV layes so that i init 2 parameters W1, W2.\n","\n","    # Output:\n","    #     params: dictionary contains initial parameters W1, W2\n","    # '''\n","    # # tf.contrib.layers.xavier_initializer(seed = 0)\n","    # # mode='FAN_IN': # Count only number of input connections.\n","    # W1a = tf.get_variable('W1a', [3, 3, 1, 32], initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', seed=0, uniform=False))\n","    # W1b = tf.get_variable('W1b', [3, 3, 32, 32], initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', seed=0, uniform=False))\n","\n","    # W2a = tf.get_variable('W2a', [3, 3, 32, 64], initializer =tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', seed=0, uniform=False))\n","    # W2b = tf.get_variable('W2b', [3, 3, 64, 64], initializer =tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', seed=0, uniform=False))\n","    \n","    # # Return params dictionary\n","    # params = {'W1a': W1a, 'W1b': W1b, \\\n","    #           'W2a': W2a, 'W2b': W2b}\n","\n","    '''Init params to build neural network using He initialization\n","    Model includes 2 CONV layes so that i init 2 parameters W1, W2.\n","\n","    Output:\n","        params: dictionary contains initial parameters W1, W2\n","    '''\n","    # tf.contrib.layers.xavier_initializer(seed = 0)\n","    # mode='FAN_IN': # Count only number of input connections.\n","    W1 = tf.get_variable('W1', [5, 5, 1, 32], initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', seed=0, uniform=False))\n","    W2 = tf.get_variable('W2', [5, 5, 32, 64], initializer =tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', seed=0, uniform=False))\n","    \n","    # Return params dictionary\n","    params = {'W1': W1, 'W2': W2}\n","    \n","    return params"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yAyZYP5dlLN1","outputId":"5c821c25-058b-4a22-fc20-f94bdc111e72","executionInfo":{"status":"ok","timestamp":1570448831394,"user_tz":-420,"elapsed":2376,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["tf.reset_default_graph()\n","# Start session\n","with tf.Session() as sess:\n","    params = init_params()\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    # eval() to access init params\n","    print(params['W1'].eval()[2, 1, 0])\n","    print(params['W2'].eval()[2, 1, 0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","[ 0.27962556  0.01836919  0.5818591   0.15281099  0.16358273 -0.33858886\n"," -0.15803637  0.01159567 -0.02905411 -0.16152051 -0.5868845   0.4466479\n","  0.1160896   0.1171051   0.2162253   0.21260528  0.24762763  0.16704114\n","  0.14749299  0.05631219 -0.11084587  0.5368453   0.5445989  -0.33338666\n","  0.22532874  0.3943117  -0.25896066 -0.47179097  0.24023701  0.00590787\n"," -0.18029146  0.45188966]\n","[-0.06015451 -0.01164385  0.0289786  -0.03375296 -0.06890791 -0.02539449\n","  0.0090615  -0.011644   -0.08910772  0.0496453  -0.01981989  0.07181952\n","  0.11395305 -0.06079784 -0.01650224  0.02331122  0.07756662 -0.00081236\n","  0.0847094  -0.00965964  0.02868275  0.04013321  0.01422175  0.06308981\n"," -0.01501739 -0.01931455 -0.0447724   0.05030658 -0.00652873  0.01529624\n","  0.00725804 -0.00831342  0.00117738 -0.06957775 -0.03392741 -0.0550753\n"," -0.05353649  0.00039461 -0.04112608  0.04801245  0.05947588 -0.00059354\n"," -0.01484878  0.04906268 -0.02981846  0.03534172 -0.08308113 -0.05523397\n","  0.06452356  0.01028943  0.03709921  0.02067373  0.07424542  0.00338509\n","  0.02428557 -0.07028407  0.02077861 -0.02020923  0.03201746  0.04543288\n","  0.01854187 -0.06355195  0.05506046 -0.00425077]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XVvkqslVlLN3"},"source":["# 5. Forward propagation"]},{"cell_type":"code","metadata":{"id":"mzcxvvVliD3I","colab_type":"code","colab":{}},"source":["def block1(X, keep_prob, W, is_training, block_number):\n","    \"\"\"\n","    Block i: CONV -> BatchNorm -> Pool -> Dropout (i = block_number)\n","\n","    Input:\n","        X: shape [m, nHi-1, nWi-1, nCi-1]\n","        keep_prob: placeholder (tf.float32) used for dropout\n","        W: parameter of CONV i (Wi)\n","        is_training: placeholder (tf.bool) used for batchnorm\n","        block_number\n","    Output:\n","        X: output of droupout layer ([m, nHi-1/srides[1], nWi-1/strides[2], nCi])\n","    \"\"\"\n","\n","    # CONVi: stride 1, padding 'SAME'\n","    with tf.variable_scope('CONV'+str(block_number), reuse = tf.AUTO_REUSE):\n","        # Zi and Ai shape: [m, nHi-1, nWi-1, nCi]\n","        X = tf.nn.conv2d(X, W, strides = [1,1,1,1], padding = 'SAME')  #Zi\n","        # RELUi: same shape as Zi\n","        X = tf.nn.relu(X) # Ai\n","        # Batch normalization\n","        X = tf.layers.batch_normalization(inputs=X, axis=-1, training=is_training)\n","\n","    with tf.variable_scope('POOL'+str(block_number), reuse = tf.AUTO_REUSE):\n","        # POOL1: ksize [1, 2, 2, 1], strides = [1, 2, 2, 1]\n","        # P1 shape: [m, nHi, nWi, nCi] = [m, nHi-1/srides[1], nWi-1/strides[2], nCi]\n","        X = tf.nn.max_pool(X, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') # Pi\n","    \n","    with tf.variable_scope('Dropout'+str(block_number), reuse = tf.AUTO_REUSE):\n","        X = tf.nn.dropout(X, keep_prob) # Di\n","\n","    return X    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQL677LwRJ_7","colab_type":"code","colab":{}},"source":["def block2(X, keep_prob, W_block, is_training, block_number):\n","    \"\"\"\n","    Block i: CONVa -> BatchNorm -> CONVb -> BatchNorm -> Pool -> Dropout (i = block_number)\n","\n","    Input:\n","        X: shape [m, nHi-1, nWi-1, nCi-1]\n","        keep_prob: placeholder (tf.float32) used for dropout\n","        W_block: tuple of parameter of CONV i (Wa, Wb)\n","        is_training: placeholder (tf.bool) used for batchnorm\n","        block_number\n","    Output:\n","        X: output of droupout layer ([m, nHi-1/srides[1], nWi-1/strides[2], nCi])\n","    \"\"\"\n","\n","    Wa, Wb = W_block[0], W_block[1]\n","\n","    # CONVia: stride 1, padding 'SAME'\n","    with tf.variable_scope('CONV'+str(block_number)+'a', reuse = tf.AUTO_REUSE):\n","        # Zi and Ai shape: [m, nHi-1, nWi-1, nCi]\n","        X = tf.nn.conv2d(X, Wa, strides = [1,1,1,1], padding = 'SAME')  #Zi\n","        # RELUi: same shape as Zi\n","        X = tf.nn.relu(X) # Ai\n","        # Batch normalization\n","        X = tf.layers.batch_normalization(inputs=X, axis=-1, training=is_training)\n","\n","    # CONVib: stride 1, padding 'SAME'\n","    with tf.variable_scope('CONV'+str(block_number)+'b', reuse = tf.AUTO_REUSE):\n","        # Zi and Ai shape: [m, nHi-1, nWi-1, nCi]\n","        X = tf.nn.conv2d(X, Wb, strides = [1,1,1,1], padding = 'SAME')  #Zi\n","        # RELUi: same shape as Zi\n","        X = tf.nn.relu(X) # Ai\n","        # Batch normalization\n","        X = tf.layers.batch_normalization(inputs=X, axis=-1, training=is_training)\n","\n","    with tf.variable_scope('POOL'+str(block_number), reuse = tf.AUTO_REUSE):\n","        # POOL1: ksize [1, 2, 2, 1], strides = [1, 2, 2, 1]\n","        # P1 shape: [m, nHi, nWi, nCi] = [m, nHi-1/srides[1], nWi-1/strides[2], nCi]\n","        X = tf.nn.max_pool(X, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') # Pi\n","    \n","    with tf.variable_scope('Dropout'+str(block_number), reuse = tf.AUTO_REUSE):\n","        X = tf.nn.dropout(X, keep_prob) # Di\n","\n","    return X    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ldkYU7BrlLN4","colab":{}},"source":["def forward_propagation(X, keep_prob, params):\n","    '''Perfome forward propagation\n","        Input -> CONV1 -> RELU1 -> POOL1 -> CONV2 -> RELU2 -> POOL2 -> FC -> DROPOUT -> SOFTMAX\n","    \n","    Input:\n","      X: shape [m, nH0, nW0, nC0]\n","      keep_prob: placeholder (tf.float32) used for dropout\n","      params: dictionary including 'W1' and 'W2' for CONV1 and CONV2 respectively\n","\n","    Output:\n","      Z4: shape [m, 10] which is a output of SOFTMAX layer (but softmax was computed by compute_cost function)\n","    '''\n","    \n","    # Get 2 params\n","    W1, W2 = params['W1'], params['W2']\n","    \n","    \n","    # CONV1: stride 1, padding 'SAME'\n","    with tf.variable_scope('CONV1', reuse = tf.AUTO_REUSE):\n","        # Z1 and A1 shape: [m, nH0, nW0, nC1]\n","        Z1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')  #[]\n","        # RELU1\n","        A1 = tf.nn.relu(Z1)\n","        \n","    with tf.variable_scope('POOL1', reuse = tf.AUTO_REUSE):\n","        # POOL1: ksize [1, 2, 2, 1], strides = [1, 2, 2, 1]\n","        # P1 shape: [m, nH1, nW1, nC1] = [m, nH0/srides[1], nW0/strides[2], nC1]\n","        P1 = tf.nn.max_pool(A1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n","        \n","    with tf.variable_scope('CONV2', reuse = tf.AUTO_REUSE):\n","        # CONV2: stride 1, padding \"SAME\"\n","        # Z2 and A2 shape: [m, nH1, nW1, nC2]\n","        Z2 = tf.nn.conv2d(P1, W2, strides = [1, 1, 1, 1], padding = 'SAME')\n","        # RELU2\n","        A2 = tf.nn.relu(Z2)\n","    \n","    with tf.variable_scope('POOL2', reuse = tf.AUTO_REUSE):\n","        # POOL2: ksize = [2, 2], strides = 2\n","        # P2 shape: [m, nH2, nW2, nC2] = [m, nH1/strides[1], nW1/strides[2], nC2]\n","        P2 = tf.nn.max_pool(A2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n","    \n","    with tf.variable_scope('FLATTEN', reuse = tf.AUTO_REUSE):\n","        # flatten feature map\n","        # F shape: [m, nH2*nW2*nC2]\n","        F = tf.contrib.layers.flatten(P2)\n","        \n","    with tf.variable_scope('FC1', reuse = tf.AUTO_REUSE):\n","        # FC1: with 1024 units and activation_fn = 'relu'\n","        # FC1 shape: [m, 1024]\n","        FC1 = tf.contrib.layers.fully_connected(F, num_outputs = 1024, activation_fn = tf.nn.relu)\n","    \n","    with tf.variable_scope('Dropout3', reuse = tf.AUTO_REUSE):\n","        D = tf.nn.dropout(FC1, keep_prob)\n","    \n","    with tf.variable_scope('FC2', reuse = tf.AUTO_REUSE):\n","        # FC2: with 10 units and set activation_fn = None because softmax function is computed separately\n","        # FC2: [m, 10]\n","        Z4 = tf.compat.v1.layers.Dense(units = 10, activation = None, name = 'Z4')(D)\n","    \n","    return Z4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TD8X8xqJXv_","colab_type":"code","colab":{}},"source":["def forward_propagation1(X, keep_prob, params, is_training):\n","    '''Perfome forward propagation\n","        Input -> CONV1 -> RELU1 -> POOL1 -> CONV2 -> RELU2 -> POOL2 -> FC -> DROPOUT -> SOFTMAX\n","    \n","    Input:\n","      X: shape [m, nH0, nW0, nC0]\n","      keep_prob: placeholder (tf.float32) used for dropout\n","      params: dictionary including 'W1' and 'W2' for CONV1 and CONV2 respectively\n","      is_training: placeholder (tf.bool) used for batchnorm\n","\n","    Output:\n","      Z4: shape [m, 10] which is a output of SOFTMAX layer (but softmax was computed by compute_cost function)\n","    '''\n","    \n","    # Get 2 block params\n","    # W1, W2 = (params['W1a'], params['W1b']), (params['W2a'], params['W2b'])\n","    W1, W2 = params['W1'], params['W2']\n","\n","    # Block 1: CONV -> BatchNorm -> Pool -> Dropout\n","    X = block1(X, keep_prob, W1, is_training, 1)\n","    \n","    # Block 2: CONV -> BatchNorm -> Pool -> Dropout\n","    X = block1(X, keep_prob, W2, is_training, 2)\n","    \n","    # Flatten -> FC -> Dropout -> FC\n","    with tf.variable_scope('FLATTEN', reuse = tf.AUTO_REUSE):\n","        # flatten feature map\n","        # F shape: [m, nH2*nW2*nC2]\n","        F = tf.contrib.layers.flatten(X)\n","        \n","    with tf.variable_scope('FC1', reuse = tf.AUTO_REUSE):\n","        # FC1: with 1024 units and activation_fn = 'relu'\n","        # FC1 shape: [m, 1024]\n","        FC1 = tf.contrib.layers.fully_connected(F, num_outputs = 1024, activation_fn = tf.nn.relu)\n","    \n","    with tf.variable_scope('Dropout3', reuse = tf.AUTO_REUSE):\n","        D = tf.nn.dropout(FC1, keep_prob)\n","    \n","    with tf.variable_scope('FC2', reuse = tf.AUTO_REUSE):\n","        # FC2: with 10 units and set activation_fn = None because softmax function is computed separately\n","        # FC2: [m, 10]\n","        Z4 = tf.compat.v1.layers.Dense(units = 10, activation = None, name = 'Z4')(D)\n","    \n","    return Z4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-ZEDbSkVlLN6","outputId":"6a40c184-329e-45a5-b2d9-4a0523c1a07e","executionInfo":{"status":"ok","timestamp":1570452172191,"user_tz":-420,"elapsed":1615,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["tf.reset_default_graph()\n","\n","with tf.Session() as sess:\n","    X, Y, keep_prob, is_training = create_placeholders(30, 30, 1, 1)\n","    params = init_params()\n","    Z4 = forward_propagation(X, keep_prob, params)\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    a = sess.run(Z4, {X: np.random.randn(2, 30, 30, 1), Y: np.random.randn(2, 1), keep_prob: 1.0, is_training:False})\n","    print(a)\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0479913748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0479913748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0479913748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0479913748>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0479913358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0479913358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0479913358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0479913358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f04799700f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f04799700f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f04799700f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f04799700f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","[[ 3.8492644  -1.8625023   2.8157747   1.7608798   4.2834272   3.8852968\n","  -1.4115161  -0.94252396 -6.434965   -3.9633875 ]\n"," [ 3.4344425  -2.9129243   3.5866396   1.9334693   4.1646852   5.9458966\n","  -1.7618097   2.4452896  -8.07802    -3.9167569 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"082ehfjclLN8"},"source":["# 6. Compute cost"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W9b4QIOAlLN9","colab":{}},"source":["def compute_cost(Z4, Y):\n","    '''\n","    Purpose: Compute cost by perfoming sofmax function first and then using cross-entropy as loss function\n","    Input:\n","      Z4: logits (not probability) with shape [m, 10]\n","      Y: labels with shape [m, 10]\n","      \n","     Output:\n","      cost (lost): cross_entropy loss (scalar)\n","    '''\n","    # Compute mean of all example to get overall cost\n","    # Loss function: Cross-entropy\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z4, labels = Y))\n","    return cost"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"I4ew-gValLOB"},"source":["# 7. Build model function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Vytc6VgzlLOB"},"source":["Ramdomize minibatches by shuffling dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fPk0-mK5lLOG"},"source":["# Fit model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TTY5FiLVlLOG","colab":{}},"source":["def fit_model(X_train, Y_train, X_val, Y_val, model_name, learning_rate = 0.001, no_epochs = 100, minibatch_size = 50, keep_prob_input = 1.0, weight_decay=5e-4, plot_learning_curve = True, model_number=0):\n","    '''\n","    Fit model includes the following steps:\n","      Init computation graph. \n","      Create placeholders for training set X and Y.\n","      Init parameters W1 and W2 for model.\n","      Forward propagation.\n","      Compute cost\n","      Create optimizer to minimize cost and tensorflow will perfome back prop for us\n","      Init global variables for model\n","      Start session and execute above operators per epoch\n","    \n","    Input:\n","      X_train [m, nH0, nW0, nC0]: training examples\n","      Y_train [m, 10]: training labels\n","      X_val [m_val, nH0, nW0, nC0]: validating examples\n","      Y_val [m_val, 10]: validating labels\n","      learning_rate: defaults 0.001 (1e-4)\n","      no_epochs: the number of epochs (default: 100)\n","      minibatch_size: defaults 50\n","      weight_decay: use for L2 regularization\n","      plot_learning_curve: If True, it will plot the learning curve of costs each 5 epochs (Default: True).\n","      model_number: number of model to use. There are 2: 0 and 1.\n","      \n","    Output:\n","      training_accuracy (%)\n","      val_accuracy (%)\n","      params: final parameters of model\n","      \n","    ***By defaults: It will plot learning curve for training examples\n","    '''\n","    \n","    tf.reset_default_graph()\n","    \n","    # Get shape of input layer\n","    m, nH0, nW0, nC0 = X_train.shape\n","    nY = Y_train.shape[-1]\n","    \n","    # costs list\n","    costs = []\n","    \n","    # Create placeholders for traing set X and Y\n","    X, Y, keep_prob, is_training = create_placeholders(nH0, nW0, nC0, nY)\n","    \n","    # init params\n","    params = init_params()\n","    \n","    # Forward prop: to get logits\n","    if model_number == 0:\n","        Z4 = forward_propagation(X, keep_prob, params)\n","    else:\n","        Z4 = forward_propagation1(X, keep_prob, params, is_training)\n","    # Compute cost\n","    cost = compute_cost(Z4, Y)\n","    \n","    # Cost function with L2 Regularization with weight_decay\n","    # regularizers = tf.nn.l2_loss(params['W1a']) + tf.nn.l2_loss(params['W1b']) + \\\n","    #                tf.nn.l2_loss(params['W2a']) + tf.nn.l2_loss(params['W2b'])\n","    regularizers = tf.nn.l2_loss(params['W1']) + tf.nn.l2_loss(params['W2'])\n","    cost = tf.reduce_mean(cost + weight_decay * regularizers)\n","\n","    # # Create cost_summary\n","    # cost_summary = tf.summary.scalar(name='Cost summary', tensor=cost)\n","\n","    # Create optimizer operators. Use AdamOptimizer to minize cost\n","    optimizer = tf.train.AdamOptimizer(learning_rate)\n","    train_op = optimizer.minimize(cost)\n","    \n","    # Create saver object to store all variables\n","    saver = tf.train.Saver()\n","    \n","    # Start Init glabal variable and Execute graph\n","    init = tf.global_variables_initializer()\n","    \n","    # Start session\n","    with tf.Session() as sess:\n","        # Create writter for log files inside session\n","        writter = tf.summary.FileWriter('../../logs/'+dtime, sess.graph)\n","\n","        # Run the initialization\n","        sess.run(init)\n","        \n","        # Loop throuh the no_epochs\n","        for i in range(no_epochs):\n","            # Renew minibatch_cost each epoch\n","            minibatch_cost = 0\n","            # Compute the number of minibatch\n","            no_minibatches = int(m / minibatch_size)\n","            \n","            \n","            # Get random minibatches\n","            minibatches = random_minibatches(X_train, Y_train, minibatch_size, seed = i)\n","            \n","            # Loop each minibatch to compute cost\n","            for minibatch in minibatches:\n","                # Unpack\n","                minibatch_X, minibatch_Y = minibatch\n","                \n","                # Run the optimizer and cost operators\n","                # Feed minibatch_X and minibatch_Y into model\n","                _, epoch_cost = sess.run([train_op, cost], feed_dict = {X: minibatch_X, Y: minibatch_Y, keep_prob: keep_prob_input, is_training: True})\n","                \n","                # Compute cost: divide epoch_cost with no_minibatches\n","                minibatch_cost += epoch_cost/no_minibatches\n","                \n","            # # Add minibatch_cost into writter for visualization\n","            # writter.add_summary(minibatch_cost, i)\n","\n","            # Print cost each 5 epochs\n","            if i % 5 == 0:\n","                print('Cost after %d: %f' %(i, minibatch_cost))\n","            \n","            # Store cost to plot learning rate\n","            costs.append(minibatch_cost)\n","        \n","        \n","        if plot_learning_curve:\n","            # Plot learning rate\n","            plt.plot(np.squeeze(costs))\n","            plt.ylabel('Cost')\n","            plt.xlabel('Epoch per 5')\n","            plt.title('Learning rate = ' + str(learning_rate))\n","            plt.show()\n","        \n","        # Calculate the training accuracy and validating accuracy\n","        predict_op = tf.arg_max(Z4, 1)\n","        correct_prediction = tf.equal(predict_op, tf.arg_max(Y, 1))\n","        \n","        # Calculate the accuracy on training set and validating set\n","        # Perfome mean to compute accuracy\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n","        \n","        # Save graph\n","        saver.save(sess, model_name, global_step = 1000)\n","        \n","        # Execute computing accuracy\n","        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, keep_prob: 1.0, is_training: False})\n","        val_accuracy = accuracy.eval({X: X_val, Y: Y_val, keep_prob: 1.0, is_training: False})\n","        \n","        print('Training accuracy:', train_accuracy)\n","        print('Validating accuracy:', val_accuracy)\n","        \n","    return train_accuracy, val_accuracy, params"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-e8Egh1yfhP","colab_type":"text"},"source":["Get datetime now presents the name of model and fit model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZvTGkd_2lLOI","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":621},"outputId":"417a0db3-814f-45a8-d2a9-d513099c002e","executionInfo":{"status":"ok","timestamp":1570452360809,"user_tz":-420,"elapsed":34288,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}}},"source":["# datetime object containing current date and time\n","now = datetime.now()\n","# dd/mm/YY H:M:S\n","dtime = now.strftime(\"%d%m%Y-%H%M%S\")\n","print(\"date and time =\", dtime)\n","\n","# Create folder dtime\n","path = 'trained_models/'+dtime\n","os.mkdir(path)\n","\n","# Change dir\n","os.chdir(path)\n","print(\"Current dir:\", os.getcwd())\n","\n","_, _, params = fit_model(X_train, Y_train, X_val, Y_val, \\\n","                         no_epochs=3, learning_rate = 1e-4, keep_prob_input=0.5, weight_decay=5e-4, \\\n","                         minibatch_size=50, plot_learning_curve=True, model_name=dtime, model_number=1)\n","\n","# change dir back to original\n","os.chdir('../..')"],"execution_count":43,"outputs":[{"output_type":"stream","text":["date and time = 07102019-124526\n","Current dir: /content/gdrive/My Drive/Senior/Computer vision in application/Lab 01/trained_models/07102019-124526\n","WARNING:tensorflow:From <ipython-input-34-1803a2b2d1c0>:22: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04774473c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04774473c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04774473c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f04774473c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0477d5b630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0477d5b630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0477d5b630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f0477d5b630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0477cb56a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0477cb56a0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0477cb56a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0477cb56a0>>: AttributeError: module 'gast' has no attribute 'Num'\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0477cb5748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","Cost after 0: 1.007979\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Training accuracy: 0.64638\n","Validating accuracy: 0.6453\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vk6RfUbldP5m","colab_type":"text"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"sS65Qzw2EnKH","colab_type":"code","colab":{}},"source":["def evaluate(model_meta_file, X_test, Y_test):\n","  tf.reset_default_graph()\n","  with tf.Session() as sess:\n","    # Load model\n","    new_saver = tf.train.import_meta_graph(model_meta_file)\n","    # Restore the value of tensors: W1, W2\n","    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n","  #   # Access value of W1, W2\n","  #   print(sess.run('W1: 0').shape)\n","  #   print(sess.run('W2: 0').shape)\n","    \n","    # Restore the graph\n","    graph = tf.get_default_graph()\n","    \n","    \n","  #   names = [n.name for n in graph.as_graph_def().node]\n","  #   for name in names:\n","  #     print(name)\n","    \n","  #   ops_name = [op.name for op in graph.get_operations()]\n","  #   print(ops_name)\n","    \n","  #   # Restore weights\n","  #   W1 = graph.get_tensor_by_name('W1: 0')\n","  #   W2 = graph.get_tensor_by_name('W2: 0')\n","    \n","    \n","    # Get tensor vars\n","    inputs = graph.get_tensor_by_name('input:0')\n","    labels = graph.get_tensor_by_name('label:0')\n","    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n","    is_training = graph.get_tensor_by_name('is_training:0')\n","\n","    # Get tensor ops\n","    Z4 = graph.get_tensor_by_name('FC2/Z4/BiasAdd:0')\n","  \n","    # # predict\n","    # logits_out = sess.run(Z4, feed_dict = {inputs: X_test, labels: Y_test, keep_prob: 1.0, is_training: False})\n","  \n","    # Calculate the training accuracy and validating accuracy\n","    predict_op = tf.arg_max(Z4, 1)\n","    correct_prediction = tf.equal(predict_op, tf.arg_max(labels, 1))\n","\n","    # Calculate the accuracy on training set and validating set\n","    # Perfome mean to compute accuracy\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n","    \n","    test_accuracy = accuracy.eval({inputs: X_test, labels: Y_test, keep_prob: 1.0, is_training: False})\n","    \n","    print('Test accuracy:', test_accuracy)\n","\n","    return test_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nlzvf18me8v5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"f851161e-43ac-4cda-8605-ec048fa750bd","executionInfo":{"status":"ok","timestamp":1570451728918,"user_tz":-420,"elapsed":2263,"user":{"displayName":"Tien Hao Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDSFMjCRFchuqzIgpO0-Ux2_JrQ12AAcYfSOG1gzA=s64","userId":"03616740043904424419"}}},"source":["# dtime = '07102019-083627'\n","# path = 'trained_models/07102019-083627'\n","evaluate(path+'/'+dtime+'-1000.meta', X_test, Y_test)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./07102019-043810-1000\n","Test accuracy: 0.8406\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.8406"]},"metadata":{"tags":[]},"execution_count":33}]}]}